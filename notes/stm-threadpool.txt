So I wrote stm.threadutils.ThreadPool, but (especially since the introduction of stm.watch) I think I can rewrite it to be somewhat more intuitive.

And I also want to rewrite it to conform to Python 3.something's concurrent.futures.Executor interface.

So, my thought is to scrap ThreadPool.schedule as an independently useful function and move all of its logic into a watcher.

(And the cool thing about that is that then a pool can be constructed against any old TList, and adding items to the TList would cause threads to be automatically spun up as necessary.)

So, the idea would go something like this:

We have a watcher function that determines (I'll get to the specifics of this bit in a bit) the number of threads short that we currently are in order to run as many tasks as we have scheduled without exceeding the maximum number of threads we're allowed.

Then we have an attached callback function that takes that number and starts that many new threads. Part of starting a thread would be to increment _live_threads (and, I'm thinking, _free_threads as well; this algorithm actually /should/ require _free_threads to be incremented immediately), and the watcher function would be written such that this would end up decreasing its number-of-thread-we-are-short to exactly 0. So the callback gets called, we start threads, the callback gets called again, and we see that we're not short any threads and so we don't start any. Note that I'll likely wrap the callback in a changes_only(according_to=operator.eq).

So, the watcher function would just be min(len(tasks) - _free_threads, _max_threads - _live_threads).

The first bit, len(tasks) - _free_threads, gives us the number of threads that we'd need to spin up to have exactly one free thread for each scheduled task.

The second bit, _max_threads - _live_threads, gives us the number of threads that we'd need to spin up to have exactly the maximum number of threads that we're allowed to have running at one time.

So then we just take the minimum of the two since we don't need more than the first one and we aren't allowed to have more than the second one, and that's it.

That also allows us to simplify the code that the workers actually run. I made it a bit more complicated to get around a scheduling issue where a bunch of tasks all scheduled on the same transaction would result in only one thread being started to handle them, but that would no longer be necessary: such a case couldn't arise, because the watcher would indicate that more tasks than free threads existed and that therefore more threads needed to be spun up.

So yeah, I think I'm going to convert the existing ThreadPool to use this approach first, before I worry about making it conform to concurrent.futures.Executor.

(Oh, also, the watch thing means that, if a new task slips into the task list between a worker marking itself as free and marking itself as no longer free, an additional thread will be spun up by the worker marking itself as no longer free to handle the task.)



So that happened and things work great, but now I've run into another issue, this one performance related: when a pool has a huge number of threads, any one of them taking a task to run causes all of the other idle ones to rescan the list of tasks to run and retry. This results in quadratic performance, which is bad. The current incarnation of examples/threadpool1.py demonstrates this quite nicely (run it with PYTHON_STM_STATS=1 to see this).

(And, in fact, because I'm subclassing ThreadPool from TObject, this also happens whenever any of the threadpool's attributes change, as for all the threads know they could have a new list in self._tasks to look at. I'm actually tempted to change TObject to use a TDict of TVars for that very reason; were I to do that, checks of things watching attributes on a TObject subclass would only be triggered when a new attribute was created, not when an existing one's value changed.)

Actually, I think I'm going to go make that change to TObject and see how that affects performance first before trying to solve the other problem.

Excellent, that had a huge performance increase: overall number of retries went from around 10,000 to just over 1000.

So that only works with performance that's that good if the pool is saturated. Otherwise an exorbitant number of retries still happen, as I mentioned above, because when one thread pulls an item off of the list to consume, all of the other threads waiting for items have to wake up, see that there aren't any items available for them, and go back to retrying.

So, I think I have an idea of how this could be fixed: instead of having a list from which threads pull tasks to work on, we have each thread (and note that I'd have to promote these worker threads to have their own class) have a field storing the task currently being run by that thread. We also have the threadpool track a queue of threads waiting for work to do.

Then we have a watcher that watches a boolean indicating whether there are any tasks that need running and whether there are any free threads on the queue. When there are tasks to run and threads to run them, the callback runs in a loop until there are no more tasks to process or no more threads to process them with. For each task, it pulls a thread off of the queue and sets the thread's current task to the task to run, and that's it.

Then each thread simply retries on its own personal task that it's supposed to be running. When it finally gets a task, it runs it, then puts itself back into the queue of threads that need work. That way, each thread only resumes from retrying when a task has been assigned to it specifically.

I think I'll try that and see how it goes.

(And a note: when I change this whole thing to be compatible with ThreadPoolExecutor, tasks will carry a future to notify of their result along with them, which will mean that threads will need to have a current future into which the result should be placed as well. Either that or I could have a wrapper around ThreadPool that supports futures by wrapping tasks with a function that sticks the result onto the future, but then future results and stats about how many tasks have been completed as offered by the thread pool wouldn't be in sync, and having them in sync might be nice.)

Oh, and, I don't think thread spawning needs to change, except that the watcher that pops a task and a thread and hands the task to the thread to run would also be responsible for decrementing the number of free threads, and the transaction that puts the thread back into the queue of free threads would also mark the thread as free. And, in fact, were I to give broadcast endpoints the ability to tell the user how many items they had on them, we wouldn't even need to track this number as we could just ask the free thread endpoint how many free threads it has.

In fact, that enhancement to BroadcastEndpoint sounds rather lovely. I think I'm going to go write it.
