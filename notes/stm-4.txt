Yet another idea for an STM rewrite...

Not as overarching as previous attempts, though.

So I just discovered a few days ago that there's a term for the sort of thing I was thinking about to allow integration of various transactional database systems with STM: two-phase commits.

Basically, such databases expose an interface with the following three functions, aside from functions used to read and write data (and start new transactions):

    rollback: Undo any changes made during this transaction.
    
    prepare: Check to make sure that changes made as part of this transaction don't conflict with changes made as part of another, committed transaction; raise an error if conflicts are detected, put the database in a state where this transaction is now guaranteed to commit otherwise.
    
    commit: Commit changes made during this transaction. This must always succeed (and must not be called before a corresponding call to prepare).

The idea, then, is that at commit time, all modified databases' prepare functions are called. If any of them fail, the rollback functions of all of the databases for which prepare has been called are invoked, and the transaction restarts. If none of them fail, then all of the databases' commit functions are invoked, and the transaction concludes.

This, of course, offers no support for retry(). I'll worry about that later.

So, the idea is that TVars act as their own little databases of sorts, with the provisio that the STM system offer them a bit of memory in which to store state. (I think this is a necessity of any database, really; most databases will want to store a reference to the particular connection being used for this particular transaction.) Reads and writes proceed as they currently do, obtaining the global lock and raising exceptions as necessary. (Transactional databases should do the same thing, namely raise an exception if queries are made whose results would be inconsistent with the results of queries already performed.) Writes would, of course, make use of transaction storage to hold the new value.

Then, come commit time, the prepare function would obtain the global lock, check to make sure the TVar has not since been modified (raising an error and releasing the global lock if it has), and then return, leaving the global lock obtained. Subsequent TVars would do the same thing (note that the global lock must be reentrant in order for this to work). Then the commit function would save the changes and release the global lock.

The rollback function, then, would do nothing more than release the global lock without saving any changes that had been made.

And actually, that would make it remarkably easy to switch to using per-TVar locks: each TVar acquires its own lock, and then the transaction system as a whole has only to ensure that TVars are prepared and committed in an order that remains consistent across differing transactions (it could, perhaps, sort them by their Python object ids) in order for that to work and not result in deadlock.

If the global lock is going to continue to be used, though, it would actually be perhaps a bit more performant to have a single "TVar database" object of sorts that's registered with the transaction when any TVars are read, and that object would have prepare and commit functions that are called only once, and thus acquire the global lock only once. That object would, then, hold a reference to each TVar and would perform validation for all of them in one fell swoop.

I actually think that's probably the way I'm going to go for now.

So I'm thinking I'd have a module, maybe stm.core, that has the core transaction logic and functions like atomically, retry, elapsed, or_else, invariant, watch, and previously. (Did I miss any?) Then I'll have a module (say stm.tvar) that provides TVars and their implementation of the rollback/prepare/commit interface exposed by stm.core. Then stm's __init__.py will re-export all of the "public facing" functions (atomically/retry/elapsed/or_else/invariant/watch/previously) from stm.core as well as TVar from stm.tvar.

So that sounds good.

So let's see... What would stm.core expose to the public other than the 7 functions listed above?

Well, there'd be an abstract class representing a transactionally managed resource. It would have three abstract functions, namely rollback, prepare, and commit.

Then there would be some sort of function that could be used to register a new transactionally managed resource with the current transaction. That would cause its prepare, commit, and rollback functions to be invoked as needed during the current transaction.

Then there would need to be some way to access transactional memory. It could be as simple as providing a means to look up a particular transactionally managed resource given a key of some sorts.

And actually, I think that'd be it.

So that sounds good.

I'll worry about TWeakRefs, retries, invariants, watches, and previously later.

So... TWeakRefs...

Well, for preexisting TWeakRefs...

The only thing they'd need to store as part of their transactional memory would be whether the TWeakRef had been read as alive at any point during the transaction.

Then commit preparation would consist solely of checking to make sure that, if the TWeakRef had been previously read as alive, it has not yet died (and raising an apprioriate exception if it has died), and that's it.

And the actual commit would do nothing.

Then, for TWeakRefs created during the course of the current transaction...

(Note that, for such TWeakRefs, a strong reference to the ref's referent is held throughout the transaction for reasons I've explained several times before.)

I think basically all that has to happen is that committing the transaction causes the strong reference to the TWeakRef's referent to be dropped.

(Actually, I think that could just be done inside prepare. Then commit does nothing whatsoever.)

Then retries are interesting...

I think there'll have to be some sort of function, maybe listen or notify_of_changes, on the transactional resource interface that's called with a function to invoke when changes to the underlying resource are detected, and a corresponding unlisten or stop_notifying_of_changes function.

Idea would be that listen would cause the passed-in function to be invoked whenever the underlying resource is modified (until a corresponding call to unlisten), or immediately if changes have happened to the resource since the transaction first began.

And, in fact, since it seems at this point like a resource is a one-off object that's created afresh for each transaction, there could just be a no-arg unlisten function that removes the sole listener registered with listen (and of course a limitation would be placed that listen can be called only once for any given resource).

Transactional storage would probably still need to be available to listen and unlisten.

So, TVarResource (the hypothetical class that manages the entire set of TVars that have been accessed during the transaction)'s implementation would acquire the global lock, see if any of the accessed TVars have been modified since the start of the transaction (calling the listener immediately if so), and then add the listener to each of the TVar's lists of listeners to notify. Then, as part of the commit function's code that actually updates its TVars' values, it would call each of their listeners. Then unlisten would just remove the listener from the TVars' lists.

I think that would work.

Watches I /think/ could be implemented similarly... Perhaps they detect what resources are added to the transaction during the run of the function to watch, then somehow there's a similar function to indicate to a resource that something's interested in changes to that resource, and if such changes happen then... Hm, watchers need to be synchronously run at the end of a transaction that causes their value to change... That makes things more difficult.

I'll think about that more tomorrow.







